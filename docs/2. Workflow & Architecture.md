\# Workflow \& Architecture



\## Data Flow  

\- Source systems: Website session logs, order management system, product catalog, refund system.  

\- Connectors: Ingest data via Microsoft Fabric connectors or APIs into the Lakehouse.  

\- Data transformation steps: Clean, deduplicate, and enrich raw data (sessions, orders, products, refunds) using MS Fabric Data Pipelines.  

\- Landing zone: Transformed data lands in MS Fabric Lakehouse for optimized storage and query performance.



\## ETL Pipeline Outline  

1\. Step 1: Ingest raw session, order, product, and refund data files into Lakehouse staging area.  

2\. Step 2: Use MS Fabric Dataflow to cleanse data, calculate derived fields (e.g., conversion rates, sales per session), and resolve keys using the provided data dictionary.  

3\. Step 3: Load refined datasets into curated Lakehouse tables ready for analysis and visualization.

## Data Quality Layer

### Quality Infrastructure
- **data_quality_log** - Detailed validation results
- **data_quality_summary** - High-level quality metrics
- **Validation Notebooks** - Automated check execution

### Quality Flow

Raw Data → Staging Tables → Quality Validation → [PASS/FAIL Decision] ↓ PASS → Transformation Layer FAIL → Alert & Investigation


### Quality Checks
- Completeness (nulls, row counts)
- Uniqueness (primary keys, duplicates)
- Validity (data types, formats, ranges)
- Business Logic (price >= cogs, binary flags)

\## Automation/CI-CD  

\- Integration with GitHub Actions for version control and pipeline deployment automation.  

\- Schedule pipelines for regular refresh of data ingestion and transformation.  

\- Trigger points: Data arrival, pipeline completion, and deployment of updated Power BI dashboards.


