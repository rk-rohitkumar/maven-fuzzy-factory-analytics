\# Workflow \& Architecture



\## Data Flow  

\- Source systems: Website session logs, order management system, product catalog, refund system.  

\- Connectors: Ingest data via Microsoft Fabric connectors or APIs into the Lakehouse.  

\- Data transformation steps: Clean, deduplicate, and enrich raw data (sessions, orders, products, refunds) using MS Fabric Data Pipelines.  

\- Landing zone: Transformed data lands in MS Fabric Lakehouse for optimized storage and query performance.



\## ETL Pipeline Outline  

1\. Step 1: Ingest raw session, order, product, and refund data files into Lakehouse staging area.  

2\. Step 2: Use MS Fabric Dataflow to cleanse data, calculate derived fields (e.g., conversion rates, sales per session), and resolve keys using the provided data dictionary.  

3\. Step 3: Load refined datasets into curated Lakehouse tables ready for analysis and visualization.



\## Automation/CI-CD  

\- Integration with GitHub Actions for version control and pipeline deployment automation.  

\- Schedule pipelines for regular refresh of data ingestion and transformation.  

\- Trigger points: Data arrival, pipeline completion, and deployment of updated Power BI dashboards.

